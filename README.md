# Ilocano-Generator PH
*A GPT-style language generator for Ilocano(ilocano), built from a low-resource corpus*

### ğŸ“Œ Overview
**Ilocano-Generator** is a project aimed at building a **generative language model for Ilocano**, a low-resource Philippine language, using modern Transformer-based architectures.

The project:
- Starts from an *Ilocano ELECTRA encoder Model (iloELECTRA)**
- Reuses its **tokenizer and vocabulary**
- Fine-tunes a **DistilGPT2 causal language model**
- Progresses from **Plain language modeling â†’ continuation â†’ instruction-lite generation**

The current focus is **high-quality Ilocano Text generation**, not yet a full chatbot.

---

## ğŸ¯ Goals
- Build a **working GPT-style Ilocano text Generator**
- Preserve Ilocano linguistic features:
  - Archaic Spelling
  - Hyphenated forms
  - Spanish loanwords
- Avoid overfitting common low-resource pitfalls
- Establish a *reproducible training pipeline* for future expansions

---

## ğŸ“‚ Project Structure
```bash
â”œâ”€â”€ configs/
â”‚ â””â”€â”€ gpt_config.json # Training hyperparameters
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ lm.txt # Plain language modeling dataset
â”‚ â”œâ”€â”€ continuation.jsonl # Promptâ€“continuation dataset
â”‚ â””â”€â”€ instruction.jsonl # Instruction-lite dataset
â”œâ”€â”€ models/
â”‚ â”œâ”€â”€ iloELECTRA_tokenizer_gen/ # Reused tokenizer from iloELECTRA
â”‚ â””â”€â”€ ilocano-gpt/ # Fine-tuned generator checkpoints
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ train_lm.py # GPT-style LM training
â”‚ â”œâ”€â”€ train_continuation.py # Continuation fine-tuning (planned)
â”‚ â””â”€â”€ train_instruction.py # Instruction-lite fine-tuning (planned)
â””â”€â”€ README.md
```
---

## ğŸ“Š Corpus Description

The original corpus contains ~470k Ilocano lines across multiple domains:

| Domain | Approx. Lines |
|------|---------------|
| General | ~437k |
| Religion | ~25k |
| Literary Texts | ~4.5k |
| Encyclopedic | ~3k |
| News | ~700 |
| Social Forums | ~1k |
| Health | ~100 |

**Cleaning rules applied:**
- Removed URLs, HTML artifacts, numeric-only lines
- Preserved archaic spellings and loanwords
- Length-based filtering for different training objectives

---

## ğŸ§  Training Phases

### Phase 1â€“3: Dataset Preparation
- Corpus cleaning & normalization
- Dataset splitting into:
  - `lm.txt` (plain LM)
  - `continuation.jsonl`
  - `instruction.jsonl`
- Domain tagging and filtering

### Phase 4: Tokenizer Extension
- Reused iloELECTRA tokenizer
- Added `<bos>` and `<eos>` tokens
- Final vocab size: **30,002**

### Phase 5: GPT-style Language Modeling (Current)
- Base model: **DistilGPT2**
- Objective: causal language modeling
- Hardware:
  - Local CPU (sanity checks)
  - Google Colab GPU (full epochs)

### Phase 6â€“7 (Planned)
- Continuation fine-tuning
- Instruction-lite fine-tuning (News, Religion, Literary)

---

## âš™ï¸ Model & Tokenization Notes

- ELECTRA-style tokens (`[CLS]`, `[SEP]`) are **disabled**
- GPT-style generation uses:
  - Explicit `<bos>` / `<eos>` tokens
  - `add_special_tokens=False`
- Padding is aligned to `eos_token` for GPT compatibility

---



